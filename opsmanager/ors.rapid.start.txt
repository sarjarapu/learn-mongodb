
############################################################
# Replace these IP addesses with your current set 
############################################################
# opsmgr set
# ip-172-31-14-242.us-west-2.compute.internal
# ip-172-31-4-99.us-west-2.compute.internal
# ip-172-31-12-151.us-west-2.compute.internal
# ec2-54-202-168-181.us-west-2.compute.amazonaws.com
# ec2-54-202-190-254.us-west-2.compute.amazonaws.com
# ec2-54-213-1-105.us-west-2.compute.amazonaws.com

# mongdb AppDB
# ec2-54-244-159-69.us-west-2.compute.amazonaws.com
# ec2-54-244-161-108.us-west-2.compute.amazonaws.com
# ec2-54-244-161-117.us-west-2.compute.amazonaws.com

# mmsGroupId=588132557f3f5b2190ad6e23
# mmsApiKey=a82c9e18ed53e9846396c16da4fe97e7
# mmsBaseUrl=http://ec2-54-202-110-174.us-west-2.compute.amazonaws.com:8080
# Load balancer: http://ec2-54-202-110-174.us-west-2.compute.amazonaws.com:8080
# opsmgr 1: http://ec2-54-202-168-181.us-west-2.compute.amazonaws.com:8080
# mongos ec2-54-202-233-130.us-west-2.compute.amazonaws.com

# Twilio SID: AC5ad073b032ef6c9befa21638878e458a
# Twilio Auth: 4e643b1cecdb98636071c381bc4d353d

# i2cssh -Xi=~/.ssh/amazonaws_rsa -c aws_ors_omgr
# i2cssh -Xi=~/.ssh/amazonaws_rsa -c aws_ors_mongo
# openssl rand 24 > /<keyPath>/gen.key

# filesystem stores & head databases

############################################################
# Install a Basic Production Deployment on RHEL or Amazon Linux
# https://docs.opsmanager.mongodb.com/v3.4/tutorial/install-basic-deployment/
############################################################

############################################################
# do not modiy anything below this 
############################################################

tee ~/.i2csshrc <<EOF
version: 2
iterm2: true
clusters:
  aws_ors_omgr:
    login: ec2-user
    hosts:
      - ec2-54-202-168-181.us-west-2.compute.amazonaws.com
      - ec2-54-202-190-254.us-west-2.compute.amazonaws.com
      - ec2-54-213-1-105.us-west-2.compute.amazonaws.com
  aws_ors_mongo:
    login: ec2-user
    hosts:
      - ec2-54-244-159-69.us-west-2.compute.amazonaws.com
      - ec2-54-244-161-108.us-west-2.compute.amazonaws.com
      - ec2-54-244-161-117.us-west-2.compute.amazonaws.com
EOF



############################################################
# Installing the ops manager MongoDB & Web
############################################################

i2cssh -Xi=~/.ssh/amazonaws_rsa -c aws_ors_omgr

# Cmd + Shift + I
sudo yum -y upgrade 


sudo tee /etc/yum.repos.d/mongodb-enterprise.repo <<EOF
[mongodb-enterprise]
name=MongoDB Enterprise Repository
baseurl=https://repo.mongodb.com/yum/amazon/2013.03/mongodb-enterprise/3.4/\$basearch/
gpgcheck=1
enabled=1
gpgkey=https://www.mongodb.org/static/pgp/server-3.4.asc
EOF

sudo yum install -y mongodb-enterprise




sudo mkdir -p /data/opsmgr/db /data/backup/db

sudo tee /data/opsmgr/mongod.conf  <<EOF 
systemLog:
   destination: file
   path: /data/opsmgr/mongod.log
   logAppend: true
storage:
   dbPath: /data/opsmgr/db
   journal:
      enabled: true
processManagement:
   fork: true
   pidFilePath: /data/opsmgr/mongod.pid
net:
   port: 27000
replication:
   replSetName: rsProd-OMgr
#security:
#   authorization: enabled
#   keyFile: /data/opsmgr/keyfile
EOF

sudo chown -R mongod:mongod /data
sudo -u mongod sh -c "echo secretsalt | openssl sha1 -sha512  | sed 's/(stdin)= //g' > /data/opsmgr/keyfile"
sudo -u mongod sh -c "chmod 400 /data/opsmgr/keyfile"

sudo -u mongod /usr/bin/mongod --config /data/opsmgr/mongod.conf 
sleep 2



mongo --port 27000 <<EOF
rs.initiate( {
    _id : "rsProd-OMgr",
    members: [ 
       { _id : 0, host : "ip-172-31-6-40.us-west-2.compute.internal:27000" },
       { _id : 1, host : "ip-172-31-1-153.us-west-2.compute.internal:27000" },
       { _id : 2, host : "ip-172-31-5-253.us-west-2.compute.internal:27000" } 
    ]
})
EOF

sleep 10

# RUN IT ON the server where you have primary 
# mongo admin --port 27000 --eval "db.createUser({user: 'superuser', pwd: 'secret', roles: ['root']});"
# db.createUser({user: 'superuser', pwd: 'secret', roles: ['root']});

mongo admin --port 27000 --eval 'db.shutdownServer({force: true})'

sudo -u mongod sh -c "sed 's/#//g' /data/opsmgr/mongod.conf > /data/opsmgr/mongod.new.conf"
sudo -u mongod sh -c "mv /data/opsmgr/mongod.new.conf /data/opsmgr/mongod.conf"

sudo -u mongod /usr/bin/mongod --config /data/opsmgr/mongod.conf 
sleep 10





# Install Ops Manager 
# https://docs.opsmanager.mongodb.com/current/tutorial/install-on-prem-with-rpm-packages/
wget https://downloads.mongodb.com/on-prem-mms/rpm/mongodb-mms-3.4.1.385-1.x86_64.rpm

sudo rpm -ivh mongodb-mms-3.4.1.385-1.x86_64.rpm

sudo vi /opt/mongodb/mms/conf/conf-mms.properties
# Goto this line and replace connection string 
# mongo.mongoUri=mongodb://127.0.0.1:27017/?maxPoolSize=150

# mongodb://superuser:secret@ip-172-31-6-40.us-west-2.compute.internal:27000,ip-172-31-1-153.us-west-2.compute.internal:27000,ip-172-31-5-253.us-west-2.compute.internal:27000/?replicaSet=rsProd-OMgr&maxPoolSize=150

# Use scp to copy the gen.key file from the /etc/mongodb-mms/ directory on the current server to the same directory on the other servers.
sudo service mongodb-mms start

# start only one of the ops manager , let it do its thing 
# copy the /etc/mongodb-mms/gen.key from one server to the rest 

# copy amazonaws_rsa to one of the server 
scp -i ~/.ssh/amazonaws_rsa ~/.ssh/amazonaws_rsa  ec2-user@ec2-54-202-110-174.us-west-2.compute.amazonaws.com:/home/ec2-user




# move the gen.key to rest of the other servers in the cluster 
# sudo scp -i ~/amazonaws_rsa /etc/mongodb-mms/gen.key  ec2-user@ip-172-31-6-40.us-west-2.compute.internal:/home/ec2-user
sudo scp -i ~/amazonaws_rsa /etc/mongodb-mms/gen.key  ec2-user@ip-172-31-1-153.us-west-2.compute.internal:/home/ec2-user
sudo scp -i ~/amazonaws_rsa /etc/mongodb-mms/gen.key  ec2-user@ip-172-31-5-253.us-west-2.compute.internal:/home/ec2-user
sudo mv gen.key /etc/mongodb-mms/gen.key
sudo chown mongodb-mms:mongodb-mms /etc/mongodb-mms/gen.key


mongo "mongodb://root:secret@ec2-54-244-159-69.us-west-2.compute.amazonaws.com:27000/?authSource=admin&replicaSet=rsProd-AppDB"



###############################################################
# Generating the load 
###############################################################

# ip-172-31-0-137.us-west-2.compute.internal
scp -i ~/.ssh/amazonaws_rsa /Users/shyamarjarapu/Code/work/mongodb/git-hub/poc-driver/bin/POCDriver.jar  ec2-user@ec2-54-244-159-69.us-west-2.compute.amazonaws.com:/home/ec2-user
scp -i ~/.ssh/amazonaws_rsa /Users/shyamarjarapu/Code/work/mongodb/git-hub/poc-driver/bin/POCDriver.jar  ec2-user@ec2-54-244-161-108.us-west-2.compute.amazonaws.com:/home/ec2-user
scp -i ~/.ssh/amazonaws_rsa /Users/shyamarjarapu/Code/work/mongodb/git-hub/poc-driver/bin/POCDriver.jar  ec2-user@ec2-54-244-161-117.us-west-2.compute.amazonaws.com:/home/ec2-user
java -jar POCDriver.jar -i 20 -k 20 -b 10 -u 20 -c "mongodb://root:secret@ec2-54-244-159-69.us-west-2.compute.amazonaws.com:27000/?authSource=admin&replicaSet=rsProd-AppDB"
java -jar POCDriver.jar -i 60 -k 30 -b 10 -c "mongodb://root:secret@ec2-54-244-159-69.us-west-2.compute.amazonaws.com:27000/?authSource=admin&replicaSet=rsProd-AppDB"





###############################################################
# Install Automation Agents
###############################################################

i2cssh -Xi=~/.ssh/amazonaws_rsa -c aws_ors_mongo
sudo yum -y upgrade 

curl -OL http://ec2-54-202-110-174.us-west-2.compute.amazonaws.com:8080/download/agent/automation/mongodb-mms-automation-agent-manager-3.2.8.1942-1.x86_64.rpm

# Confirm the size is same across all servers
# ls -ltr mongodb-mms-automation-agent-manager-3.2.8.1942-1.x86_64.rpm  | awk '{print $5}'

sudo rpm -U mongodb-mms-automation-agent-manager-3.2.8.1942-1.x86_64.rpm
# sudo service mongodb-mms-automation-agent stop
# sudo rpm -e mongodb-mms-automation-agent-manager
# sudo rm /etc/mongodb-mms/automation-agent.config.rpmsave
# sudo cat /etc/mongodb-mms/automation-agent.config


sudo cp /etc/mongodb-mms/automation-agent.config /tmp/automation-agent.orig.config
sudo sed "s/mmsGroupId=/mmsGroupId=588132557f3f5b2190ad6e23/g" /etc/mongodb-mms/automation-agent.config | \
    sed "s/mmsApiKey=/mmsApiKey=a82c9e18ed53e9846396c16da4fe97e7/g" | \
    sed "s/mmsBaseUrl=/mmsBaseUrl=http:\/\/ec2-54-202-110-174.us-west-2.compute.amazonaws.com:8080/g" | \
    tee /tmp/automation-agent.config
sudo cp /tmp/automation-agent.config /etc/mongodb-mms/automation-agent.config
sudo cat /etc/mongodb-mms/automation-agent.config

sudo mkdir -p /data
sudo chown mongod:mongod /data
sudo service mongodb-mms-automation-agent start





# Configuring the backup daemon & headdb

# Find out where the backup daemon is running & ssh there 
sudo mkdir -p /backup/headdb
sudo chown -R mongodb-mms:mongodb-mms /backup/headdb









# Uninstall MongoDB
sudo service mongod stop
sudo yum -y erase $(rpm -qa | grep mongodb-enterprise)
sudo rm -rf /var/log/mongodb
sudo rm -rf /var/lib/mongo

# Connecting to MongoS
mongo ec2-54-202-233-130.us-west-2.compute.amazonaws.com

# Run on Laptop Uploading the jar file
# scp -i ~/.ssh/amazonaws_rsa ~/Code/work/mongodb/git-hub/poc-driver/bin/POCDriver.jar  ec2-user@ec2-54-187-204-32.us-west-2.compute.amazonaws.com:/home/ec2-user

# Download jar file on servers
# scp -i ~/.ssh/amazonaws_rsa ~/Code/work/mongodb/git-hub/poc-driver/bin/POCDriver.jar  ec2-user@ec2-54-187-204-32.us-west-2.compute.amazonaws.com:/home/ec2-user
wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=0B4KRgMp8k0BieHpHQWZkT1g5Y1k' -O POCDriver.jar


ssh -i ~/.ssh/amazonaws_rsa ec2-user@ec2-54-187-28-219.us-west-2.compute.amazonaws.com
java -jar POCDriver.jar -i 20 -k 20 -b 10 -u 20 -c "mongodb://ip-172-31-33-163.us-west-2.compute.internal:27017/"

java -jar POCDriver.jar -i 20 -k 20 -b 10 -u 20 -c "mongodb://ip-172-31-33-164.us-west-2.compute.internal:27017/"


# java -jar POCDriver.jar -i 20 -k 20 -b 10 -u 20 -c "mongodb://ip-172-31-9-250.us-west-2.compute.internal:27017/"
# java -jar POCDriver.jar -i 20 -k 20 -b 10 -u 20 -c "mongodb://ip-172-31-9-251.us-west-2.compute.internal:27017/"
# state should be: writes is not an empty list

https://docs.opsmanager.mongodb.com/v2.0/tutorial/configure-monitoring-munin-node/

sudo yum install -y munin-node
sudo service munin-node start

sudo mkdir -p /backup/headdb
sudo chown -R mongodb-mms:mongodb-mms /backup/headdb


###############################################################
# Install Backup Agent 
# link https://docs.opsmanager.mongodb.com/v2.0/tutorial/install-backup-agent-with-rpm-package/
###############################################################

curl -OL http://ec2-54-202-110-174.us-west-2.compute.amazonaws.com:8080/download/agent/backup/mongodb-mms-backup-agent-latest.x86_64.rpm
sudo rpm -U mongodb-mms-backup-agent-latest.x86_64.rpm

sudo cp /etc/mongodb-mms/backup-agent.config /tmp/backup-agent.orig.config
sudo sed "s/mmsGroupId=/mmsGroupId=588132557f3f5b2190ad6e23/g" /etc/mongodb-mms/backup-agent.config | \
    sed "s/mmsApiKey=/mmsApiKey=a82c9e18ed53e9846396c16da4fe97e7/g" | \
    sed "s/mmsBaseUrl=/mmsBaseUrl=http:\/\/ec2-54-202-110-174.us-west-2.compute.amazonaws.com:8080/g" | \
    tee /tmp/backup-agent.config
sudo cp /tmp/backup-agent.config /etc/mongodb-mms/backup-agent.config
sudo cat /etc/mongodb-mms/backup-agent.config
sudo service mongodb-mms-backup-agent start





# Questions 
# https://mongodb--c.na7.visual.force.com/apex/Console_CaseView?id=500A000000W8ENaIAN&sfdc.override=1
# https://support.mongodb.com/case/00421662

How many replica sets and sharded clusters will be monitored/managed by Ops Manager?
24 replica sets per environment

How many replica sets and sharded clusters will be backed up by Ops Manager?
24

How much HA/redundancy do you want in the deployment? A typical production deployment has three Ops Manager Application database servers and another three Blockstore database servers for Ops Manager Backup (if used).
Backups (if applicable):
Will depend on requirements, assume recommendation

Would you prefer to keep your backups in Blockstore or Filesystem Format? Please see this page for a description of both options.
Haven't decided. 
>>>>>  Filesystem

For each replica set and shard, what is the:
Oplog/day per replica set (GB) ~5mb
>>>>>> is that / hr??
File size per replica set (GB)250GB
How compressible is the data? Is it text, videos, binaries?
text


What is your expected growth over the next 6, 12, and 18 months? If it is somewhat difficult to procure hardware at your organization, we recommend sizing up for more than you will need.
when we add sharing we should be able to reduce.
>>>>>>> ?

Retention:
What is the Point In Time Restore requirement in hours? (default is 48)
4hrs

How frequently do you want your snapshots taken? (default is 24 hrs, other options are 6, 8, and 12)?
2hrs
The lowest time between snapshots that we support is 6 hours. There are a lot of moving parts involved in collecting a snapshot so this is intended to reduce the amount of background load on the Ops Manager components. The oplog of each primary node is streamed to the Backup Daemons between each snapshot, this provides a point in time restore option to any point between each snapshot for a standard deployment. For sharded clusters, checkpoints can be collected at intervals of 15, 30 or 60 minutes to give you an extra restore point between snapshots.

For the purpose of this calculation I will select 6 hours for the snapshot frequency, the oplog stream can be used for point in time restores between the snapshots. When you add sharding you can then create checkpoints as discussed above.

How many of these snapshots should be kept? (default is 2, so spanning the last 48 hours)
1
As we need to collect a snapshot once every 6 hours, we would recommend a minimum of 5 of these snapshots are kept. This will allow you a point in time recovery of 24 hours. If you wish for a greater point in time recovery option this will need to be increased, however as you have specified only 4 hours for point in time recovery I believe 24 hours will be sufficient.



What is the required number of Daily, Weekly, and Monthly snapshots to keep? (default is 5 which includes weekly snapshots for 2 weeks, and monthly snapshots for 1 month)
1 week

How long would you like to store the oldest snapshot? We are able to store information for up to 13 months, however, the retention period for snapshots can dramatically affect your total data size.
3 weeks
The minimum we would recommend here is 4 weeks, please let me know if this is suitable.


Do you desire high availability, a minimal configuration, or something in between?
high availability


I Plan on having 4 Ops managers (1 in each datacenter). Depending on the Datacenter they are in the will have a minimum of 3 replicas and a maximum of 6.
File System Storage Snapshots seems most likely
6hrs will have to do.
Recommendation of 5 is fine.
4 weeks is fine as well.



# Can I use LDAP?	Yes but this must be set up BEFORE/during installation. It cannot be configured or added later.
https://docs.opsmanager.mongodb.com/v3.4/tutorial/install-basic-deployment/

